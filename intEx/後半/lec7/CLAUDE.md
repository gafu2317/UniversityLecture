この課題は、**PyTorchを使用して簡単なTransformerを作成することを目標**としています。

課題は大きく3つのパートに分かれています。

---

### **2 課題の概要**

1.  **PyTorchの行列計算の確認 (Tensor.py)**
    *   `torch.tensor`クラスを用いた行列計算の基本を理解します。
    *   `Tensor.py`で定義されている変数（`x`, `y`, `M`, `N`, `K`, `A`, `V`）を使って様々な演算を行い、その結果を考察します。
    *   この課題では、**コードの提出は不要**です。
    *   注記として、1次元配列の`print`表示は縦ベクトルとして、2次元配列の`print`表示は数式上は行列表記として扱います。

2.  **埋め込みとAttentionの理解 (Embedding.py)**
    *   アルファベット一文字をトークンとして、簡単な文字列を2次元空間に埋め込み、Positional EncodingとSelf-Attentionを実行する検証コード（`Embedding.py`）を扱います。
    *   学習は行わず、初期重みでの順伝播のみを行います。
    *   スライドの数式は単一の入力文を想定していますが、プログラムはミニバッチを想定しているため、**バッチサイズが最初の次元に入る**点に注意が必要です。

3.  **Transformerによるひらがな文章生成 (Trans.py)**
    *   Transformer（Encoderのみ）を用いてひらがなの文章生成を行うプログラム（`Trans.py`）を作成します。
    *   学習データとして、ひらがなで書かれた昔話（`mukashi-banashi.txt`）を使用します。
    *   ひらがな一文字をトークンとして扱い、ユーザーからの8文字のひらがな入力の続きを生成します。
    *   課題2で作成した`PositionalEncoding`クラスと`SelfAttention`クラスの内容を`Trans.py`に転記して使用します。

---

### **各課題の詳細**

#### **1. PyTorchの行列計算の確認 (Tensor.py)**
以下の問いに答え、空欄を埋める形式です。
*   **(a) ベクトル`x`と`y`の要素ごとの演算**：和、差、積（要素ごと）、商（要素ごと）の結果をベクトル表記で埋めます。
*   **(b) 行列`M`とベクトル`x`の要素ごとの演算**：和、差、積（要素ごと）、商（要素ごと）の結果を行列表記で埋めます。ただし、ベクトルと行列の要素ごとの積は$\odot$、商は$\oslash$を使います。
*   **(c) 3次元テンソル`N`と2次元テンソル`K`の要素ごとの和**：`N[0,:,:]`を`N1`、`N[1,:,:]`を`N2`としたときの`N + K`の結果を記述し、その配列サイズを答えます。
*   **(d) テンソル`N`と`M`の行列積、および線形層の適用**：
    *   `N @ M.transpose(0,1)`の結果を記述し、その配列サイズを答えます。
    *   `nn.Linear`で定義される線形層`linear`の重みパラメータ`W`のサイズを答え、`linear(M)`および`linear(N)`の結果を記述し、それぞれの配列サイズを答えます。
*   **(e) 3次元テンソル`A`と`V`の行列積**：`A @ V`の結果を記述し、その配列サイズを答えます。
*   **(f) 3次元テンソル`N`と`V`の転置と行列積**：`V.transpose(1,2)`の結果を記述し、`N @ V.transpose(1,2)`の結果を記述し、その配列サイズを答えます。
*   **(g) `F.softmax`の`dim`引数の違い**：`F.softmax(N, dim=0)`, `F.softmax(N, dim=1)`, `F.softmax(N, dim=2)`の結果を観察し、なぜそのようになるか数行で説明します。

#### **2. 埋め込みとAttentionの理解 (Embedding.py)**
以下の問いに答え、一部はコードの完成も求められます。
*   **(a) `input_idx`テンソルの表現内容**：配列サイズを明記し、各次元が何に対応しているか説明します。
*   **(b) `nn.Embedding`の機能**：`token_embedding = nn.Embedding(char_size, dim_embed)`が何を行うか、`char_size`と`dim_embed`の意味を含めて説明します。また、`x = token_embedding(input_idx)`で作成される`x`のサイズと各次元の意味を説明します。
*   **(c) `embed-1.pdf`の図の説明**：図を説明し、**重複して同じ位置にある点がなぜそうなるのかを明記**します。
*   **(d) `PositionalEncoding`クラスの完成**：`pe`に適切な値を入れ、Positional Encodingを完成させます。完成後、生成される`embed-2.pdf`の図を説明し、**`embed-1.pdf`との重複の変化を明記**します。
*   **(e) `SelfAttention`クラスの完成**：`forward`関数にSelf-attentionの順伝播計算を実装します。バッチの存在により、キー・クエリー・バリューが3次元配列となっていることに注意し、各軸の意味を理解して進めることが求められます。完成後、生成される`embed-3.pdf`の図を説明し、**`embed-2.pdf`との重複の変化を明記**します。

#### **3. Transformerによるひらがな文章生成 (Trans.py)**
以下の問いに答え、Transformerのクラスを完成させます。
*   **(a) 損失関数`loss_fn(logits, y)`の計算内容**：コードを読んだり、変数を`print`したりして理解し、説明します。
*   **(b) `Transformer`クラスの完成と文章生成**：
    *   `Transformer`クラスの`__init__`と`forward`関数を完成させます。
    *   アーキテクチャは**「Token埋め込み → Positional encoding → Self-attention (single head) → 全結合層」**とし、**全結合層は「線形層 (output: 100次元) → ReLU → 線形層 (output: char_size次元) 」**とします。
    *   実行後、変数`input_str`に8文字のひらがな文字列を代入し（例: `"ももからうまれた"`）、`generate_text(model, input_str)`で文章生成を行います。
    *   **損失の推移を示す`loss.pdf`の図を提出**し、**学習後に生成された文章の例を5つ記載**します。

---

この課題を進めるにあたり、`Tensor.py`、`Embedding.py`、`Trans.py`などのファイルが展開したフォルダ内に全て含まれていることを確認してください。また、`torch`と`torchvision`がインストールされている必要があります。