from typing import Optional
from pathlib import Path
import time

import matplotlib.pyplot as plt
import pandas as pd

from q_learning import QTableAgent
from offline_env import OfflineEnv


def train(
    env,
    num_steps,
    agent,
    eval_env,
    eval_interval=10**3,
    eval_steps=100,
    log_q: Optional[Path] = None,
):
    # è©•ä¾¡çµæœã®è¨˜éŒ²å‘ã‘ã«åˆæœŸåŒ–
    start_timestamp = time.time()
    eval_rewards = []
    elapse_time = []
    steps = []

    state, _ = env.reset()

    for step in range(1, num_steps + 1):
        action = agent.select_action(state)
        next_state, reward, _, _, _ = env.step(action)

        agent.update(state, action, reward, next_state, False)
        state = next_state

        # è©•ä¾¡ã¨ãè¨˜éŒ²
        if step % eval_interval == 0:
            eval_reward_sum = 0
            _eval_step = 0
            eval_state, _ = eval_env.reset()
            while _eval_step < eval_steps:
                action = agent.greedy(eval_state)
                next_state, reward, _, _, _ = eval_env.step(action)
                eval_state = next_state
                eval_reward_sum += reward
                _eval_step += 1

            print(f"Evaluation: {step=}, {eval_reward_sum=}")
            eval_rewards.append(eval_reward_sum)
            elapse_time.append(time.time() - start_timestamp)
            steps.append(step)

    # Qãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä¿å­˜ã—è¨˜éŒ²ã‚’è¿”ã™
    if log_q is not None:
        agent.save_q(log_q)
    return eval_rewards, elapse_time, steps


if __name__ == "__main__":
    # å­¦ç¿’ç”¨ã®ç’°å¢ƒ
    env = OfflineEnv(
        # ä»®æƒ³ã®ğŸã®å¯¿å‘½ï¼[1,6)ã®é–“ã®æ•´æ•°å€¤ã®å¯¿å‘½ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«è¨­å®šï¼å¯¿å‘½ãŒåˆ‡ã‚ŒãŸã‚‰ï¼Œåˆ¥ã®å ´æ‰€ã«ãƒ©ãƒ³ãƒ€ãƒ ã«é…ç½®ï¼
        life_range=(35, 36)
    )
    # è©•ä¾¡ç”¨ã®ç’°å¢ƒ
    eval_env = OfflineEnv(life_range=(35, 36))

    agent = QTableAgent(
        env.observation_space,
        env.action_space,
        alpha=0.1,  # å­¦ç¿’ç‡
        gamma=0.1,  # å‰²å¼•ç‡
        epsilon=0.1,  # æ¢ç´¢ç‡
    )

    # å­¦ç¿’ã®å®Ÿè¡Œï¼Œè©•ä¾¡çµæœã®csvãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆï¼Œãã®å¯è¦–åŒ–
    eval_rewards, elapse_time, steps = train(
        env,
        eval_env=eval_env,
        agent=agent,
        num_steps=10**2,  # å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æ•°
        eval_interval=10**1,  # è©•ä¾¡é–“éš”
        eval_steps=20,  # 1å›ã®è©•ä¾¡ã‚ãŸã‚Šã«è¨ˆæ¸¬ã™ã‚‹ã‚¹ãƒ†ãƒƒãƒ—æ•°
        log_q=Path(".") / "test_q_trained",  # Qãƒ†ãƒ¼ãƒ–ãƒ«ã®æ›¸ãå‡ºã—å…ˆ
    )

    # csvãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã™
    df = pd.DataFrame({"step": steps, "eval_rewards": eval_rewards})
    df.to_csv("test_eval.csv")

    # å¯è¦–åŒ–ã™ã‚‹
    plt.plot(steps, eval_rewards)
    plt.xlabel("#step")
    plt.ylabel("Evaluated summed reward")
    plt.show()
